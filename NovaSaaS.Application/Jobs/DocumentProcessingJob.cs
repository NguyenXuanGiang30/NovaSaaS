using NovaSaaS.Application.Interfaces.AI;
using NovaSaaS.Application.Services.AI;
using NovaSaaS.Domain.Entities.AI;
using NovaSaaS.Domain.Interfaces;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Configuration;
using NovaSaaS.Application.Interfaces;

namespace NovaSaaS.Application.Jobs
{
    /// <summary>
    /// DocumentProcessingJob - Job x·ª≠ l√Ω t√†i li·ªáu AI (chunking + embedding) trong background.
    /// Fire-and-forget khi upload document.
    /// </summary>
    public class DocumentProcessingJob
    {
        private readonly IUnitOfWork _unitOfWork;
        private readonly IChunkingService _chunkingService;
        private readonly IEmbeddingService _embeddingService;
        private readonly IConfiguration _configuration;
        private readonly ILogger<DocumentProcessingJob> _logger;
        private readonly INotificationService _notificationService;

        public DocumentProcessingJob(
            IUnitOfWork unitOfWork,
            IChunkingService chunkingService,
            IEmbeddingService embeddingService,
            IConfiguration configuration,
            ILogger<DocumentProcessingJob> logger,
            INotificationService notificationService)
        {
            _unitOfWork = unitOfWork;
            _chunkingService = chunkingService;
            _embeddingService = embeddingService;
            _configuration = configuration;
            _logger = logger;
            _notificationService = notificationService;
        }

        /// <summary>
        /// X·ª≠ l√Ω document: chunking ‚Üí embedding ‚Üí l∆∞u segments.
        /// </summary>
        /// <param name="tenantId">ID c·ªßa tenant ƒë·ªÉ g·ª≠i th√¥ng b√°o</param>
        /// <param name="documentId">ID c·ªßa document c·∫ßn x·ª≠ l√Ω</param>
        /// <param name="tenantSchemaName">Schema name c·ªßa tenant ƒë·ªÉ switch context</param>
        public async Task ProcessDocumentAsync(Guid tenantId, Guid documentId, string tenantSchemaName)
        {
            _logger.LogInformation("üîÑ Starting document processing: {DocumentId} for Tenant {TenantId}", 
                documentId, tenantId);

            string fileName = "Unknown";

            try
            {
                // Note: Background Jobs c·∫ßn ƒë·∫£m b·∫£o DbContext ƒëang connect ƒë√∫ng Schema.
                // ·ªû ƒë√¢y gi·∫£ s·ª≠ UnitOfWork/DbContext ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh scope b·ªüi Hangfire Activator 
                // ho·∫∑c middleware, n·∫øu ch∆∞a th√¨ c·∫ßn set schema th·ªß c√¥ng.
                // Tuy nhi√™n, logic n√†y s·∫Ω ph·ª• thu·ªôc v√†o implementation c·ªßa DatabaseInitializer/TenantService.
                
                // L·∫•y document
                var document = await _unitOfWork.KnowledgeDocuments.GetByIdAsync(documentId);
                if (document == null)
                {
                    _logger.LogWarning("Document not found: {DocumentId}", documentId);
                    return;
                }

                fileName = document.FileName;

                // Notify Started
                await _notificationService.NotifyDocumentProcessingStartedAsync(tenantId, documentId, fileName);

                // Update status ‚Üí Processing
                document.Status = DocumentProcessingStatus.Embedding;
                document.UpdateAt = DateTime.UtcNow;
                _unitOfWork.KnowledgeDocuments.Update(document);
                await _unitOfWork.CompleteAsync();

                // L·∫•y text content
                var textContent = document.ExtractedContent;
                if (string.IsNullOrEmpty(textContent))
                {
                    await _notificationService.NotifyDocumentProcessingFailedAsync(tenantId, documentId, fileName, "Document is empty");
                    return;
                }

                // Chunking
                var chunkSize = _configuration.GetValue<int>("RAGSettings:ChunkSize", 800);
                var chunkOverlap = _configuration.GetValue<int>("RAGSettings:ChunkOverlap", 150);
                var chunks = _chunkingService.ChunkText(textContent, chunkSize, chunkOverlap);

                _logger.LogInformation("üìÑ Document chunked: {ChunkCount} chunks", chunks.Count);

                // T·∫°o embeddings v√† l∆∞u segments
                var segments = new List<DocumentSegment>();
                for (int i = 0; i < chunks.Count; i++)
                {
                    var chunk = chunks[i];
                    
                    try
                    {
                        var embedding = await _embeddingService.GenerateEmbeddingAsync(chunk.Content);
                        
                        var segment = new DocumentSegment
                        {
                            Id = Guid.NewGuid(),
                            DocumentId = documentId,
                            SegmentIndex = i,
                            Content = chunk.Content,
                            Embedding = new Pgvector.Vector(embedding),
                            TokenCount = _chunkingService.EstimateTokenCount(chunk.Content),
                            StartPosition = chunk.StartPosition,
                            EndPosition = chunk.EndPosition,
                            CreateAt = DateTime.UtcNow
                        };

                        segments.Add(segment);
                        
                        // Notify Progress every 10 chunks or 20%
                        if (chunks.Count > 10 && (i + 1) % (chunks.Count / 5) == 0)
                        {
                            await _notificationService.NotifyDocumentProcessingProgressAsync(
                                tenantId, documentId, fileName, i + 1, chunks.Count);
                        }
                    }
                    catch (Exception ex)
                    {
                        _logger.LogError(ex, "Failed to create embedding for chunk {Index}", i);
                    }
                }

                // L∆∞u t·∫•t c·∫£ segments
                foreach (var segment in segments)
                {
                    _unitOfWork.DocumentSegments.Add(segment);
                }

                // Update document status
                document.Status = DocumentProcessingStatus.Completed;
                document.SegmentCount = segments.Count;
                document.ProcessedAt = DateTime.UtcNow;
                document.UpdateAt = DateTime.UtcNow;
                _unitOfWork.KnowledgeDocuments.Update(document);

                await _unitOfWork.CompleteAsync();

                // Notify Completed
                await _notificationService.NotifyDocumentProcessingCompletedAsync(tenantId, documentId, fileName, segments.Count);

                _logger.LogInformation("‚úÖ Document processed successfully: {DocumentId}", documentId);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "‚ùå Document processing failed: {DocumentId}", documentId);
                
                await _notificationService.NotifyDocumentProcessingFailedAsync(tenantId, documentId, fileName, ex.Message);

                // Update DB status if possible
                try
                {
                    var document = await _unitOfWork.KnowledgeDocuments.GetByIdAsync(documentId);
                    if (document != null)
                    {
                        document.Status = DocumentProcessingStatus.Failed;
                        document.ErrorMessage = ex.Message;
                        _unitOfWork.KnowledgeDocuments.Update(document);
                        await _unitOfWork.CompleteAsync();
                    }
                }
                catch { /* Ignore */ }

                throw;
            }
        }
    }
}
